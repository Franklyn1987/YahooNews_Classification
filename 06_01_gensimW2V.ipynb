{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora,models,similarities,utils\n",
    "import logging\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from keras.preprocessing import text,sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取训练数据\n",
    "def getTrainSet(inFile):\n",
    "    # 文章标题集\n",
    "    title_set = []\n",
    "    # 训练集\n",
    "    train_set=[]\n",
    "    # 读入训练数据\n",
    "    f=open(inFile)\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        article = line.replace('\\n','').split('\\t')\n",
    "        title = article[0]\n",
    "        title_set.append(title)\n",
    "        content = article[1:]\n",
    "        train_set.append(content)\n",
    "    f.close()\n",
    "    return (title_set,train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 如果输入分布在硬盘上的多个文件中, 文件的每一行是一个句子, 那么可以逐个文件, 逐行的处理输入\n",
    "# # 这样处理可以不必一次性把所有句子都载入内存\n",
    "# class MySentences(object):\n",
    "#     def __init__(self, dirname):\n",
    "#         self.dirname = dirname\n",
    "#     def __iter__(self):\n",
    "#         for fname in os.listdir(self.dirname):\n",
    "#             for line in open(os.path.join(self.dirname, fname)):\n",
    "#                 yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 引入新的数据在线训练,写入到新的模型里面\n",
    "# def trainNew(newdata,modelFile,newModelFile):\n",
    "#     new_model = models.Word2Vec.load(modelFile)\n",
    "#     new_model.train(newdata)\n",
    "#     # 存储模型\n",
    "#     new_model.save(newModelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算单词相似度\n",
    "def caluSimilarity(model,word):\n",
    "    print u\"*** %s ***\" %(word)\n",
    "    for x in model.most_similar(word,topn=10):\n",
    "        print u\"(%s,%s)\" % (x[0].encode('utf-8'),x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练word2vec\n",
    "def trainModel(inFile,modelFile,vecFile):\n",
    "    # 读入数据    \n",
    "    title_set,data_set = getTrainSet(inFile)\n",
    "    \n",
    "    # 训练\n",
    "    # 少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "    # size = 神经网络的隐藏层的单元数 default value is 100\n",
    "    # workers= 控制训练的并行:default = 1 worker (no parallelization) 只有在安装了Cython后才有效\n",
    "    model = models.Word2Vec(data_set,min_count=5,window=10,size = 50,workers=2)\n",
    "    \n",
    "    # 存储模型\n",
    "    model.save(modelFile)\n",
    "    \n",
    "    # 存储vector\n",
    "    model.wv.save_word2vec_format(vecFile, binary=True) \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 测试\n",
    "def testModel(modelFile,vecFile):\n",
    "    # 装载模型\n",
    "    model = models.Word2Vec.load(modelFile)\n",
    "    \n",
    "    word_vec = model.wv.load_word2vec_format(vecFile, binary=True) \n",
    "    \n",
    "    print word_vec.vocab[u\"金利\"]\n",
    "    # 获取相似词语\n",
    "    caluSimilarity(model,\"金利\")\n",
    "    \n",
    "    caluSimilarity(model,\"エネルギー\")\n",
    "    \n",
    "    caluSimilarity(model,\"外相\")\n",
    "    \n",
    "    caluSimilarity(model,\"家計\")\n",
    "    \n",
    "    caluSimilarity(model,\"株式\")\n",
    "    #获得词向量\n",
    "#     print model[\"銀行\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把原始文本转化为由词汇表索引表示的矩阵\n",
    "def fastBuildSeq(inFile,outFile,modelFile,vecFile):\n",
    "    # 读入数据\n",
    "    title_set,data_set = getTrainSet(inFile)\n",
    "    \n",
    "    # 装载模型\n",
    "    model = models.Word2Vec.load(modelFile)\n",
    "    word_vec = model.wv.load_word2vec_format(vecFile, binary=True) \n",
    "    \n",
    "    # 使用dir(object)查看对象的属性\n",
    "    # 对每一个文章做转换      \n",
    "    # 注意：由于word2vec的向量在训练的时候用的是unicode的编码，\n",
    "    # 所以在字典里面匹配key的时候，需要把key转化为unicode的编码，使用decode('utf-8')\n",
    "    transfrom = []\n",
    "    for news in data_set:\n",
    "        trs_news = [word_vec.vocab[w.decode('utf-8')].index for w in news if w.decode('utf-8') in word_vec.vocab]\n",
    "#         # --- 调试\n",
    "#         trs_news = []\n",
    "#         for w in news:\n",
    "#             if w.decode('utf-8') in word_vec.vocab:\n",
    "#                 print \"in vocab = \",w.decode('utf-8')\n",
    "#                 trs_news.append((word_vec.vocab[w.decode('utf-8')].index,w))\n",
    "#         # --\n",
    "        transfrom.append(trs_news)\n",
    "    \n",
    "#     for x in transfrom:\n",
    "#         print x\n",
    "    \n",
    "    # 对文字序列做补齐 ，补齐长度=最长的文章长度 ，补齐在最后，补齐用的词汇默认是词汇表index=0的词汇，也可通过value指定\n",
    "    # 训练好的w2v词表的index = 0 对应的词汇是空格\n",
    "    result_data = sequence.pad_sequences(transfrom,maxlen=200,padding='post')\n",
    "    \n",
    "#     print result_data\n",
    "    \n",
    "#     for x in result_data:\n",
    "#         print x\n",
    "    \n",
    "    np.save(outFile,result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "#     inFile = \"./data/test.txt\"\n",
    "    inFile = \"./data/all.txt\"\n",
    "    outFile = \"./data/train_cnn.npy\"\n",
    "#     modelFile = \"./model/w2v.mdl\"\n",
    "    modelFile = \"./model/w2v_dim50.mdl\"\n",
    "#     vecFile = \"./model/vector.bin\"\n",
    "    vecFile = \"./model/vector_dim50.bin\"\n",
    "    targetData = \"./data/target.npy\"\n",
    "    \n",
    "    # 读入标签\n",
    "#     target = np.load(targetData)\n",
    "    \n",
    "    # 训练模型\n",
    "#     trainModel(inFile,modelFile,vecFile)\n",
    "    \n",
    "    # 测试模型\n",
    "    testModel(modelFile,vecFile)\n",
    "    \n",
    "    # 把分词以后的文本转化为供CNN训练的数据文件\n",
    "#     fastBuildSeq(inFile,outFile,modelFile,vecFile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
