{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import sys\n",
    "import codecs\n",
    "import csv\n",
    "import MeCab\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, pprint\n",
    "def pp(obj):\n",
    "    pp = pprint.PrettyPrinter(indent=4, width=160)\n",
    "    str = pp.pformat(obj)\n",
    "    return re.sub(r\"\\\\u([0-9a-f]{4})\", lambda x: unichr(int(\"0x\"+x.group(1),16)), str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取停用词一览\n",
    "def readStopWord(ssfile):\n",
    "    ss = []\n",
    "    fr = open(ssfile,\"r\")\n",
    "    for line in fr.readlines():\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            ss.append(line)\n",
    "    fr.close()\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取CSV文件\n",
    "def readDataFile(fileName):\n",
    "    with open(fileName,\"r+\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        # skip header\n",
    "#         header = next(reader)   \n",
    "#         print \"Read article:\",fileName\n",
    "        # 读取内容\n",
    "        for line in reader:\n",
    "#             print pp(line)\n",
    "            return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取词汇本体\n",
    "def get_surfaces(node):\n",
    "    words = []\n",
    "    while node:\n",
    "#         word  = node.surface.decode('utf-8')\n",
    "        word  = node.surface\n",
    "        words.append(word)\n",
    "        node = node.next\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取词性\n",
    "def get_nouns(node):\n",
    "    nouns = []\n",
    "    while node:\n",
    "#         noun = node.feature.split(\",\")[0].decode('utf-8')\n",
    "        noun = node.feature.split(\",\")[0]\n",
    "        nouns.append(noun)\n",
    "        node = node.next\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 利用词性筛选词汇\n",
    "def select_feature(data,attr):\n",
    "    result = []\n",
    "    pattern1 = re.compile(r'[^0-9]+')\n",
    "    pattern2 = re.compile(ur'[^０１２３４５６７８９]+')\n",
    "    pattern3 = re.compile(ur'[^\\[\\]!$%&\\'\\\"\\(\\):\\-\\.,/;=]+')\n",
    "    pattern4 = re.compile(ur'[^！　％、。）※「」（＞～』＜？－．♪【⇒∞★〇・]+')\n",
    "    for (w,a) in data:\n",
    "        if a in attr:\n",
    "#             print w,a\n",
    "            # 过滤所有数字\n",
    "            tmp = w.decode('utf-8')\n",
    "            matcher1 = re.match(pattern1,w)\n",
    "            matcher2 = re.match(pattern2,tmp)\n",
    "            matcher3 = re.match(pattern3,tmp)\n",
    "            matcher4 = re.match(pattern4,tmp)\n",
    "            if (matcher1 is not None) and (matcher2 is not None) and (matcher3 is not None) and (matcher4 is not None):\n",
    "                result.append((w,a))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把筛选以后的结果写入CSV文件\n",
    "def writeOutput(fileName,data,mode):\n",
    "    with open(fileName,mode) as csvfile: \n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "#         line = \"\"\n",
    "#         for w in data:\n",
    "#             line = line + w + \"\\t\"\n",
    "#         writer.writerows( line  + \"\\n\")\n",
    "        \n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 输出结果有两个文件\n",
    "# 全量分词结果文件：ALLWORD_filename\n",
    "# 词性筛选结果文件：mode_file (默认筛选名词 mode = \"N\")\n",
    "# 筛选 形容詞 ：mode 指定 \"A\"\n",
    "# 筛选 动词 ：mode 指定 \"V\"\n",
    "# 例如：筛选 名词，形容词的场合 :mode = \"NA\"\n",
    "def processFile(inPath,outPath,stopwords,mode=\"N\"):\n",
    "    \n",
    "    # 获取文件列表\n",
    "    fileList = os.listdir(inPath)\n",
    "    \n",
    "    for f in fileList:\n",
    "        # 编辑文件路径\n",
    "        fileName = inPath + f\n",
    "        # 获取文件内容\n",
    "        content = readDataFile(fileName)\n",
    "        \n",
    "    \n",
    "        # 定义分词器\n",
    "        mt = MeCab.Tagger('mecabrc')\n",
    "#         node = mt.parseToNode(content[1].encode('utf-8'))\n",
    "        node = mt.parseToNode(content[0])\n",
    "        words, nouns = get_surfaces(node), get_nouns(node)\n",
    "        wordList = zip(words,nouns)\n",
    "        \n",
    "#         # 全词分词结果写入CSV\n",
    "#         outFile_allword = outPath + \"ALLWORD_\" + f\n",
    "#         writeOutput(outFile_allword,wordList,\"wb\")\n",
    "  \n",
    "        # 编辑筛选词性的分词输出文件名\n",
    "        outFile_noun = outPath + mode + \"_\" + f\n",
    "        \n",
    "        attr = [u'名詞']\n",
    "        \n",
    "        if \"A\" in mode:\n",
    "            # 筛选形容詞结果写入CSV\n",
    "            attr.append(u'形容詞')\n",
    "  \n",
    "        if \"V\" in mode:\n",
    "            # 筛选動詞结果写入CSV\n",
    "            attr.append(u'動詞')\n",
    "        \n",
    "        # 筛选名词结果写入CSV\n",
    "        selectList = select_feature(wordList,attr)\n",
    "        outWord = []\n",
    "        for (w,a) in selectList: \n",
    "            if w not in stopwords:\n",
    "                outWord.append((w,a))\n",
    "#                 outWord.append(w)\n",
    "#                 print w\n",
    "#         writeOutput(outFile_noun,outWord,\"wb\")\n",
    "        writeOutput(outPath,outWord,\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 修改抓取的文本文件路径\n",
    "    input_path = \"/home/hadoop/DataSencise/text_data/comment/\"\n",
    "    # 修改分词结果文件路径\n",
    "    output_path = \"/home/hadoop/DataSencise/text_data/test/test.txt\"\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    ssfile = \"./conf/Japanese_stopwords.txt\"\n",
    "    ssList = readStopWord(ssfile)\n",
    "    # 筛选名词    \n",
    "#     processFile(input_path,output_path,\"N\")\n",
    "    # 筛选名词和形容次\n",
    "#     processFile(input_path,output_path,\"NA\")\n",
    "    # 筛选名词和形容次，动词\n",
    "    processFile(input_path,output_path,ssList,\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
